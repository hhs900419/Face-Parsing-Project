{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from configs import *\n",
    "from tester import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confifurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the class obj\n",
    "configs = Configs()\n",
    "SEED = configs.seed\n",
    "cudnn.enabled = True\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "run the command to create ground truth mask\n",
    "```shell\n",
    "python preprocess.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tranform = Compose({\n",
    "    RandomCrop(448),\n",
    "    RandomHorizontallyFlip(p=0.5),\n",
    "    AdjustBrightness(bf=0.1),\n",
    "    AdjustContrast(cf=0.1),\n",
    "    AdjustHue(hue=0.1),\n",
    "    AdjustSaturation(saturation=0.1)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = configs.root_dir\n",
    "image_dir = os.path.join(ROOT_DIR, 'CelebA-HQ-img')\n",
    "\n",
    "train_indices = set()\n",
    "indices_file_pth = os.path.join(ROOT_DIR, 'train.txt')\n",
    "with open(indices_file_pth, 'r') as file:\n",
    "    train_indices = set(map(int, file.read().splitlines()))\n",
    "    \n",
    "sample_indices = list(range(len(os.listdir(image_dir))))\n",
    "test_indices = [idx for idx in sample_indices if idx not in train_indices]\n",
    "\n",
    "# Split indices into training and validation sets\n",
    "train_indices = list(train_indices)\n",
    "if configs.debug:\n",
    "    train_indices = train_indices[:100]         ###############################   small training data for debugging   ###########################################\n",
    "VAL_SIZE = configs.val_size\n",
    "train_indices, valid_indices = train_test_split(train_indices, test_size=VAL_SIZE, random_state=SEED)\n",
    "print(len(train_indices))\n",
    "print(len(valid_indices))\n",
    "print(len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset ###\n",
    "trainset = CelebAMask_HQ_Dataset(root_dir=ROOT_DIR, \n",
    "                            sample_indices=train_indices,\n",
    "                            mode='train', \n",
    "                            tr_transform=train_tranform)\n",
    "validset = CelebAMask_HQ_Dataset(root_dir=ROOT_DIR, \n",
    "                                sample_indices=valid_indices, \n",
    "                                mode = 'val')\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(validset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a DataLoader to get batches of samples\n",
    "dataloader = DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Get a batch of samples\n",
    "for images, masks in dataloader:\n",
    "    # Visualize each sample in the batch\n",
    "    print(f'b_img_size: {images.shape}')\n",
    "    print(f'b_mask_size: {masks.shape}')\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i].permute(1, 2, 0).numpy()  # Convert PyTorch tensor to NumPy array and rearrange dimensions\n",
    "        mask = masks[i].numpy()\n",
    "        # Plot the image and mask side by side\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Image')\n",
    "\n",
    "        plt.subplot(2, 4, i + 5)\n",
    "        plt.imshow(mask, cmap='gray')  # Assuming masks are grayscale\n",
    "        # plt.imshow(mask)  # Assuming masks are grayscale\n",
    "        plt.title('Mask')\n",
    "    plt.show()\n",
    "    break  # Only visualize the first batch for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataloader ###\n",
    "BATCH_SIZE = configs.batch_size\n",
    "N_WORKERS = configs.n_workers\n",
    "\n",
    "# sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    shuffle = True,\n",
    "                    num_workers = N_WORKERS,\n",
    "                    pin_memory = True,\n",
    "                    drop_last = True)\n",
    "\n",
    "valid_loader = DataLoader(validset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    shuffle = False,\n",
    "                    num_workers = N_WORKERS, \n",
    "                    pin_memory = True,\n",
    "                    drop_last = True)\n",
    "print(f\"training data: {len(train_indices)} and validation data: {len(valid_indices)} loaded succesfully ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check input/output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Unet(n_channels=3, n_classes=19)\n",
    "# batch = torch.randn(1,3,448,448)    \n",
    "# result = model(batch) #It is your img input\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init model ###\n",
    "DEVICE = configs.device\n",
    "model = Unet(n_channels=3, n_classes=19).to(DEVICE)\n",
    "print(\"Model Initialized !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyper params ###\n",
    "EPOCHS = configs.epochs\n",
    "LR = configs.lr\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0001, amsgrad=False)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, min_lr=1e-6, verbose=True)  # goal: maximize miou\n",
    "criterion = DiceLoss()\n",
    "SAVEPATH = configs.model_path\n",
    "SAVENAME = configs.model_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training ###\n",
    "Trainer( model=model, \n",
    "    trainloader=train_loader,\n",
    "    validloader=valid_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler, \n",
    "    device=DEVICE,\n",
    "    savepath=SAVEPATH, \n",
    "    savename=SAVENAME).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tester Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tester import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset and testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataloader ###\n",
    "BATCH_SIZE = configs.batch_size\n",
    "N_WORKERS = configs.n_workers\n",
    "\n",
    "testset = CelebAMask_HQ_Dataset(root_dir=ROOT_DIR,\n",
    "                            sample_indices=test_indices,\n",
    "                            mode='test')\n",
    "\n",
    "test_loader = DataLoader(testset,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    shuffle = False,\n",
    "                    num_workers = N_WORKERS, \n",
    "                    pin_memory = True,\n",
    "                    drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = configs.device\n",
    "SAVEPATH = configs.model_path\n",
    "OUTPUT_DIR = configs.cmp_result_dir\n",
    "MODEL_WEIGHT = configs.model_weight\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(n_channels=3, n_classes=19).to(DEVICE)\n",
    "model.load_state_dict(torch.load(os.path.join(SAVEPATH , MODEL_WEIGHT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing\n",
    "Tester(model=model, \n",
    "       testloader=test_loader, \n",
    "       criterion=criterion, \n",
    "       device=DEVICE).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 60 samples of comparision result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize\n",
    "    cmap = np.array([(0,  0,  0), (204, 0,  0), (76, 153, 0),\n",
    "                         (204, 204, 0), (51, 51, 255), (204, 0, 204), (0, 255, 255),\n",
    "                         (51, 255, 255), (102, 51, 0), (255, 0, 0), (102, 204, 0),\n",
    "                         (255, 255, 0), (0, 0, 153), (0, 0, 204), (255, 51, 153),\n",
    "                         (0, 204, 204), (0, 51, 0), (255, 153, 51), (0, 204, 0)],\n",
    "                        dtype=np.uint8)\n",
    "    \n",
    "    to_tensor = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "    image_dir = os.path.join(ROOT_DIR, 'CelebA-HQ-img') \n",
    "    mask_dir = os.path.join(ROOT_DIR, 'mask')    \n",
    "\n",
    "    test_dataset =[]\n",
    "    for i in range(len([name for name in os.listdir(image_dir) if osp.isfile(osp.join(image_dir, name))])):\n",
    "        img_path = osp.join(image_dir, str(i)+'.jpg')\n",
    "        label_path = osp.join(mask_dir, str(i)+'.png')\n",
    "        test_dataset.append([img_path, label_path])\n",
    "\n",
    "    # inference again in file order\n",
    "    # for i in tqdm(range(0, len(train_indices))):\n",
    "    for i in tqdm(range(0, len(test_indices), 100)):\n",
    "        idx = test_indices[i]\n",
    "        if configs.debug:\n",
    "            idx = valid_indices[i]\n",
    "            idx = train_indices[i]\n",
    "        img_pth, mask_pth = test_dataset[idx]\n",
    "\n",
    "        image = Image.open(img_pth).convert('RGB')\n",
    "        image = image.resize((512, 512), Image.BILINEAR)\n",
    "        mask = Image.open(mask_pth).convert('L')\n",
    "\n",
    "        image = to_tensor(image).unsqueeze(0)\n",
    "        gt_mask = torch.from_numpy(np.array(mask)).long()\n",
    "\n",
    "        pred_mask = model(image.to(DEVICE))     # predict\n",
    "        pred_mask = pred_mask.data.max(1)[1].cpu().numpy()  # Matrix index  (1,19,h,w) => (1,1,h,w)\n",
    "        \n",
    "        image = image.squeeze(0).permute(1,2,0)     # (1,3,h,w) -> (h,w,3)\n",
    "        pred_mask = pred_mask.squeeze(0)            # (1,h,w) -> (h,w)\n",
    "\n",
    "        # generate color mask image\n",
    "        color_gt_mask = cmap[gt_mask]\n",
    "        color_pr_mask = cmap[pred_mask]\n",
    "        \n",
    "        plt.figure(figsize=(13, 6))\n",
    "        image = Image.open(img_pth).convert('RGB')      # we want the image without normalization for plotting\n",
    "        image = image.resize((512, 512), Image.BILINEAR)\n",
    "        img_list = [image, color_pr_mask, color_gt_mask]\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.imshow(img_list[i])\n",
    "        plt.show()\n",
    "        # plt.savefig(f\"{OUTPUT_DIR}/result_{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
